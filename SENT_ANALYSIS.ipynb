{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3SKzZDa7i8aQ",
        "eHL_LcA3lHSG",
        "hTd_E2T4mzCy",
        "zhGh4SA0Hd_U",
        "UFmJdcM74VSu",
        "D-0E3eFT5spu"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3wkIQ9as-hF"
      },
      "source": [
        "# PART A: Basic Content Analysis with Twitter Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgbu2Lmz-v9y"
      },
      "source": [
        "Scraping qualitative data from the web is less intimidating than it sounds, but you will need the right tools to begin your analysis. In this section, we will run through the basics of Tweet scraping using the Twitter API Search Endpoint and discuss how to tailor your results to your specific needs.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "Please note you will be unable to execute commands from the first section. An output file (ca_prop_tweets.csv) is provided in the datafolder for use in section 2.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "There are a few reasons why you are unable to follow along in this first section. First, our class Python environment is missing several key packages needed to interact with Twitter and make meaning from tweets. These packages include tweepy (one of the more widely used packages to interact with Twitter's API endpoints) and textblob (a natural language processing package with several useful text analysis methods). We could easily install these to the class environment, but this would require that we take a few unnecessary risks. Second, to interact with Twitter via their API, you will need consumer and access tokens so that Twitter can monitor your search/stream requests. We do not have time to create Twitter developer accounts today, unfortunately, but you can find more information on this using the link below.\n",
        "\n",
        "<br>\n",
        "\n",
        "https://developer.twitter.com/en\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "Other useful links:\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "*   Twitter Search API parameters: https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/api-reference/get-search-tweets\n",
        "*   Tweet object data dictionary: https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/overview/tweet-object\n",
        "*   tweepy reference for interacting with Twitter API: http://docs.tweepy.org/en/v3.5.0/api.html#tweepy-api-twitter-api-wrapper\n",
        "*   textblob reference for NPL processes: https://textblob.readthedocs.io/en/dev/api_reference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SKzZDa7i8aQ"
      },
      "source": [
        "## 1: A brief introduction to web scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXxPkLe_iyel"
      },
      "source": [
        "from tweepy import API\n",
        "from tweepy import Cursor\n",
        "from tweepy.streaming import StreamListener\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy import Stream\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qex2JJ0ljgG9"
      },
      "source": [
        "consumer_key =\n",
        "consumer_secret =\n",
        "access_token =\n",
        "access_secret ="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeEJ0xkSjtc9"
      },
      "source": [
        "# in order to interact with any Twitter API endpoint, you will need to prove your identity...\n",
        "# the tweepy package initiates this process via the OAuthHandler class\n",
        "\n",
        "class TwitterAuthenticator():\n",
        "\n",
        "    def authenticate_twitter_app(self):\n",
        "        auth = OAuthHandler(consumer_key,\n",
        "                            consumer_secret)\n",
        "        auth.set_access_token(access_token,\n",
        "                              access_secret)\n",
        "        return auth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmtt19C-j3KN"
      },
      "source": [
        "### 1.1: Accessing the Twitter Search API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJHqM3TYj4lF"
      },
      "source": [
        "# the two primary tasks you are likely to use with tweepy are (a) streaming live tweets and (b) searching for existing tweets\n",
        "# the primary function of this class is to interact with the SEARCH endpoint\n",
        "\n",
        "class TwitterClient():\n",
        "\n",
        "    # __init__() is a built-in function for every class that executes whenever the function is called\n",
        "    # here, we are saying hello to the Twitter API\n",
        "\n",
        "    def __init__(self, twitter_user=None):\n",
        "        self.auth = TwitterAuthenticator().authenticate_twitter_app()\n",
        "        self.twitter_client = API(self.auth)\n",
        "\n",
        "        self.twitter_user = twitter_user\n",
        "\n",
        "    # here, we are requesting access to tweets from the Twitter API\n",
        "\n",
        "    def get_twitter_client_api(self):\n",
        "      return self.twitter_client\n",
        "\n",
        "    # this function is the one to pay attention to...Alter this using the tweepy documentation above\n",
        "    # here, we are applying the search method using tweepy's Cursor object to search for tweets\n",
        "    # our criteria is limited to a search query, geographic information and the number of tweets we'd like to request (see tweepy documentation)\n",
        "\n",
        "    def get_search_results(self, query, geo, num_tweets):\n",
        "        tweets = []\n",
        "        for tweet in Cursor(self.twitter_client.search,\n",
        "                            q = query,\n",
        "                            lang = 'en',\n",
        "                            geocode = geo,\n",
        "                            count = num_tweets).items(num_tweets):\n",
        "          tweets.append(tweet)\n",
        "        return tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VODzdZH6kUSu"
      },
      "source": [
        "# tweet data comes in an efficient, but difficult-to-read format called JSON\n",
        "# this class allows us to run a basic sentiment analysis and create a legible pandas DataFrame for our tweets\n",
        "\n",
        "class TweetAnalyzer():\n",
        "\n",
        "  # these two functions set us up to run a basic sentiment analysis method from textblob\n",
        "  # we are cleaning the data by running a regular expression command and subsequently running a\n",
        "  # basic machine learning algorithm to estimate the polarity (pos/neg/neutral) of each tweet\n",
        "\n",
        "  def clean_tweet(self, tweet):\n",
        "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
        "\n",
        "  def analyze_sentiment(self, tweet):\n",
        "        analysis = TextBlob(self.clean_tweet(tweet))\n",
        "\n",
        "        if analysis.sentiment.polarity > 0:\n",
        "            return 1\n",
        "        elif analysis.sentiment.polarity == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return -1\n",
        "\n",
        "  # this function creates a pandas dataframe and uses list comprehension code to derive\n",
        "  # values from the list of tweets we generate. We can access other aspects of a tweet using\n",
        "  # different root-level attributes. See Tweet Object data dictionary link above\n",
        "\n",
        "  def tweets_to_data_frame(self, tweets):\n",
        "    df = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['tweets'])\n",
        "\n",
        "    df['date'] = np.array([tweet.created_at.strftime(\"%m/%d/%Y %H:%M:%S\") for tweet in tweets])\n",
        "\n",
        "    df['date'] = df['date'].astype(object)\n",
        "\n",
        "    df['len'] = np.array([len(tweet.text) for tweet in tweets])\n",
        "    df['latlon'] = np.array([tweet.coordinates for tweet in tweets])\n",
        "    df['user_loc'] = np.array([tweet.user.location for tweet in tweets])\n",
        "    df['user_handle'] = np.array([tweet.user.screen_name for tweet in tweets])\n",
        "    df['followers'] = np.array([tweet.user.followers_count for tweet in tweets])\n",
        "    df['favorites'] = np.array([tweet.favorite_count for tweet in tweets])\n",
        "    df['retweets'] = np.array([tweet.retweet_count for tweet in tweets])\n",
        "\n",
        "    df = df.set_index(np.array([tweet.id for tweet in tweets]))\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMZtJR1jkb7x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "39f786fa-0fe7-4974-ff24-f5f71449fa33"
      },
      "source": [
        "# don't worry too much about this if statement...this is us telling the python interpreter\n",
        "# to only run the following functions/statements if they appear in this script\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  # Question for you: what does it look like we are searching for here?\n",
        "\n",
        "  query = '\"delta\" -filter:retweets'\n",
        "  geo = '34.042201,-118.245854,100km'\n",
        "  num_tweets = 50\n",
        "\n",
        "  twitter_client = TwitterClient()\n",
        "  tweet_analyzer = TweetAnalyzer()\n",
        "\n",
        "  tweets = twitter_client.get_search_results(query, geo, num_tweets)\n",
        "  df = tweet_analyzer.tweets_to_data_frame(tweets)\n",
        "\n",
        "  # Another question for you: what are we appending to our dataframe and why might these\n",
        "  # pieces of information be interesting to us?\n",
        "\n",
        "  df['tb_sentiment'] = np.array([tweet_analyzer.analyze_sentiment(tweet) for tweet in df['tweets']])\n",
        "  df['query_term'] = query\n",
        "  df['scrape_time'] = datetime.now().strftime(\"%m/%d/%Y %H:%M:%S\")\n",
        "\n",
        "  df.index.name = 'id'\n",
        "\n",
        "print(df.head)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method NDFrame.head of                                                                 tweets  \\\n",
            "id                                                                       \n",
            "1314325575608098816  @nancyjdavidson @Delta I say book it! Mine cur...   \n",
            "1314325180148113408  @hklegacy @Delta Thanks Hann, glad you liked m...   \n",
            "1314324111246594049  @morgfair @GeorgeTakei @Delta  you need to ban...   \n",
            "1314323830072987649  Delta Airlines flight #DAL335 spotted at 38,00...   \n",
            "1314323312013725698  Hurricane #Delta mini-series:\\nPart 1 - small ...   \n",
            "1314317343279865856  Natural Gas Price Fundamental Daily Forecast â€“...   \n",
            "1314316153150271488  Delta Airlines flight #DAL884 spotted at 30,30...   \n",
            "1314314245316595713  Trump â€œtotal shut down of everything from Chin...   \n",
            "1314313662237032448  Hurricane Delta to strike Louisiana Friday as ...   \n",
            "1314313393830916096  Delta Airlines flight #DAL757 spotted at 4,800...   \n",
            "1314313253468561408  Delta Airlines flight #DAL855 spotted at 19,20...   \n",
            "1314312107236573184  Hurricane #Delta is making an appearance on th...   \n",
            "1314311089748762624  Delta Airlines flight #DAL1439 spotted at 24,3...   \n",
            "1314310985671344128  Delta Airlines flight #DAL391 spotted at 11,77...   \n",
            "1314310456102715393  \"Delta\" Dawn what is going on: Hurricane \"Delt...   \n",
            "1314309403319836673  Delta Airlines flight #DAL735 spotted at 29,47...   \n",
            "1314308962389377025  The late action film actor Steve James (The Wa...   \n",
            "1314308461690122240  Delta Airlines flight #DAL1367 spotted at 23,4...   \n",
            "1314308251068948480  Delta Airlines flight #DAL904 spotted at 27,92...   \n",
            "1314308209142718464  The damage from #HurricaneDelta will be more c...   \n",
            "1314306591781392385  Delta Airlines flight #DAL427 spotted at 225 f...   \n",
            "1314306457630695424  Delta Airlines flight #DAL326 spotted at 31,70...   \n",
            "1314304812339523584  Delta Airlines flight #DAL346 spotted at 1,300...   \n",
            "1314304583011835920  Louisiana tells meteorologist Jim Cantore to '...   \n",
            "1314304413138329608  I mean, I'm in bed, so not really bracing. \\n\\...   \n",
            "1314304122951094273  Delta Airlines flight #DAL323 spotted at 350 f...   \n",
            "1314301051470704645  Delta Airlines flight #DAL1750 spotted at 29,0...   \n",
            "1314300291240525825  Delta Airlines flight #DAL1173 spotted at 14,7...   \n",
            "1314300157144440833  Delta Airlines flight #DAL365 spotted at 36,40...   \n",
            "1314299231834759168  Hurricane Delta. I will see you guys there. Di...   \n",
            "1314297942715621377  Information about Hurricane Delta #GoogleCrisi...   \n",
            "1314297075228499969  @lanilutar @AmericanAir Alaska &amp; southwest...   \n",
            "1314296791270006784  Delta Airlines flight #DAL784 spotted at 7,825...   \n",
            "1314296714740690944  He lives in a 65-year-old cruise ship idling i...   \n",
            "1314296551888621568  The soror could have not made the joke about h...   \n",
            "1314295715917570048  TONIGHT: Catch the encore of \"Papa Tango Sierr...   \n",
            "1314293141923487744  Delta Airlines flight #DAL778 spotted at 3,475...   \n",
            "1314291775608160257                               @Delta I sent via DM   \n",
            "1314290127846342657  \"A dozen dopes from a local militia plot to ki...   \n",
            "1314289130386255872  My prev cofellow @BenLDamd just got his study ...   \n",
            "1314288139884589056  @IQ45sociopath @JoJoFromJerz I have 2 Belgian ...   \n",
            "1314287950658760709               @Delta please help re. Lounge access   \n",
            "1314286729877094400  @SteveSchmidtSES @AmericanAir @ATT @LCGpr @Ame...   \n",
            "1314286728220413952  While GK Delta has to work in secret, #AlterNa...   \n",
            "1314286090128297984  Delta Airlines flight #DAL806 spotted at 28,42...   \n",
            "1314285710652788736  Delta Airlines flight #DAL791 spotted at 375 f...   \n",
            "1314285223564115968  Delta Airlines flight #DAL1049 spotted at 16,6...   \n",
            "1314284585379745793  Cameron LNG to shut Louisiana #LNG export plan...   \n",
            "1314283772934717440  Delta Airlines flight #DAL846 spotted at 650 f...   \n",
            "1314281017906143233  @wadeflash6 @naveen4721 @puppy_trades Just say...   \n",
            "\n",
            "                                    date  len latlon             user_loc  \\\n",
            "id                                                                          \n",
            "1314325575608098816  10/08/2020 22:03:30   98   None           Orange, CA   \n",
            "1314325180148113408  10/08/2020 22:01:56  102   None           Orange, CA   \n",
            "1314324111246594049  10/08/2020 21:57:41   94   None      Los Angeles, CA   \n",
            "1314323830072987649  10/08/2020 21:56:34   77   None      Los Angeles, CA   \n",
            "1314323312013725698  10/08/2020 21:54:31  140   None     Huntington Beach   \n",
            "1314317343279865856  10/08/2020 21:30:48  130   None      Los Angeles, CA   \n",
            "1314316153150271488  10/08/2020 21:26:04   77   None      Los Angeles, CA   \n",
            "1314314245316595713  10/08/2020 21:18:29  140   None      Los Angeles, CA   \n",
            "1314313662237032448  10/08/2020 21:16:10  102   None     Santa Monica, CA   \n",
            "1314313393830916096  10/08/2020 21:15:06   76   None      Los Angeles, CA   \n",
            "1314313253468561408  10/08/2020 21:14:33   77   None      Los Angeles, CA   \n",
            "1314312107236573184  10/08/2020 21:09:59  129   None         Redlands, CA   \n",
            "1314311089748762624  10/08/2020 21:05:57   78   None      Los Angeles, CA   \n",
            "1314310985671344128  10/08/2020 21:05:32   77   None      Los Angeles, CA   \n",
            "1314310456102715393  10/08/2020 21:03:26  140   None            Azusa, CA   \n",
            "1314309403319836673  10/08/2020 20:59:15   77   None      Los Angeles, CA   \n",
            "1314308962389377025  10/08/2020 20:57:29  110   None      Los Angeles, CA   \n",
            "1314308461690122240  10/08/2020 20:55:30   78   None      Los Angeles, CA   \n",
            "1314308251068948480  10/08/2020 20:54:40   77   None      Los Angeles, CA   \n",
            "1314308209142718464  10/08/2020 20:54:30  140   None           Irvine, CA   \n",
            "1314306591781392385  10/08/2020 20:48:04   74   None      Los Angeles, CA   \n",
            "1314306457630695424  10/08/2020 20:47:32   77   None      Los Angeles, CA   \n",
            "1314304812339523584  10/08/2020 20:41:00   76   None      Los Angeles, CA   \n",
            "1314304583011835920  10/08/2020 20:40:05  137   None      New York and LA   \n",
            "1314304413138329608  10/08/2020 20:39:25  127   None  The Dirty South, LA   \n",
            "1314304122951094273  10/08/2020 20:38:16   74   None      Los Angeles, CA   \n",
            "1314301051470704645  10/08/2020 20:26:03   78   None      Los Angeles, CA   \n",
            "1314300291240525825  10/08/2020 20:23:02   78   None      Los Angeles, CA   \n",
            "1314300157144440833  10/08/2020 20:22:30   77   None      Los Angeles, CA   \n",
            "1314299231834759168  10/08/2020 20:18:50   62   None      Los Angeles, CA   \n",
            "1314297942715621377  10/08/2020 20:13:42   79   None  The Dirty South, LA   \n",
            "1314297075228499969  10/08/2020 20:10:15   95   None    Orange County, CA   \n",
            "1314296791270006784  10/08/2020 20:09:08   76   None      Los Angeles, CA   \n",
            "1314296714740690944  10/08/2020 20:08:49  130   None        ðŸŒ´Los AngelesðŸ’¥   \n",
            "1314296551888621568  10/08/2020 20:08:11  140   None      Los Angeles, CA   \n",
            "1314295715917570048  10/08/2020 20:04:51  139   None        Hollywood, CA   \n",
            "1314293141923487744  10/08/2020 19:54:38   76   None      Los Angeles, CA   \n",
            "1314291775608160257  10/08/2020 19:49:12   20   None                   LA   \n",
            "1314290127846342657  10/08/2020 19:42:39  140   None      Los Angeles, CA   \n",
            "1314289130386255872  10/08/2020 19:38:41  140   None   San Bernardino, CA   \n",
            "1314288139884589056  10/08/2020 19:34:45  140   None  Southern California   \n",
            "1314287950658760709  10/08/2020 19:34:00   36   None                   LA   \n",
            "1314286729877094400  10/08/2020 19:29:09  140   None   Marina del Rey, CA   \n",
            "1314286728220413952  10/08/2020 19:29:08  140   None         Torrance, CA   \n",
            "1314286090128297984  10/08/2020 19:26:36   77   None      Los Angeles, CA   \n",
            "1314285710652788736  10/08/2020 19:25:06   74   None      Los Angeles, CA   \n",
            "1314285223564115968  10/08/2020 19:23:10   78   None      Los Angeles, CA   \n",
            "1314284585379745793  10/08/2020 19:20:38   98   None       El Segundo, CA   \n",
            "1314283772934717440  10/08/2020 19:17:24   74   None      Los Angeles, CA   \n",
            "1314281017906143233  10/08/2020 19:06:27  140   None      Los Angeles, CA   \n",
            "\n",
            "                         user_handle  followers  favorites  retweets  \\\n",
            "id                                                                     \n",
            "1314325575608098816         hklegacy        300          0         0   \n",
            "1314325180148113408   nancyjdavidson         28          1         0   \n",
            "1314324111246594049     abbymcnormal          3          0         0   \n",
            "1314323830072987649         laxradar        158          0         0   \n",
            "1314323312013725698         surfline     189248          2         2   \n",
            "1314317343279865856  TheNatGasUpdate        108          0         0   \n",
            "1314316153150271488         laxradar        158          0         0   \n",
            "1314314245316595713        markowitz       2043          1         3   \n",
            "1314313662237032448     GrantWGraves       1790          0         0   \n",
            "1314313393830916096         laxradar        158          0         0   \n",
            "1314313253468561408         laxradar        158          0         0   \n",
            "1314312107236573184          kekenes        928          4         1   \n",
            "1314311089748762624         laxradar        158          0         0   \n",
            "1314310985671344128         laxradar        158          0         0   \n",
            "1314310456102715393       DuluthLPD6         79          0         0   \n",
            "1314309403319836673         laxradar        158          0         0   \n",
            "1314308962389377025    XanaduFitness       1088          0         0   \n",
            "1314308461690122240         laxradar        158          0         0   \n",
            "1314308251068948480         laxradar        158          0         0   \n",
            "1314308209142718464         geofffox       5040          0         0   \n",
            "1314306591781392385         laxradar        158          0         0   \n",
            "1314306457630695424         laxradar        158          0         0   \n",
            "1314304812339523584         laxradar        158          0         0   \n",
            "1314304583011835920         YahooEnt     627861          1         2   \n",
            "1314304413138329608   CrankyAssCajun       5372          6         0   \n",
            "1314304122951094273         laxradar        158          0         0   \n",
            "1314301051470704645         laxradar        158          0         0   \n",
            "1314300291240525825         laxradar        158          0         0   \n",
            "1314300157144440833         laxradar        158          0         0   \n",
            "1314299231834759168      ImBuzzBunny        135          0         0   \n",
            "1314297942715621377   CrankyAssCajun       5372          1         0   \n",
            "1314297075228499969         bobzulka       2919          0         0   \n",
            "1314296791270006784         laxradar        158          0         0   \n",
            "1314296714740690944       shelbygrad      15451          2         0   \n",
            "1314296551888621568    kreativesoul_       1855          7         2   \n",
            "1314295715917570048    TheatreofArts       1097          2         0   \n",
            "1314293141923487744         laxradar        158          0         0   \n",
            "1314291775608160257        TheLALook        336          0         0   \n",
            "1314290127846342657  JoshGibsonJokes        257          0         0   \n",
            "1314289130386255872      briantleemd        267          2         0   \n",
            "1314288139884589056       malinutt10       1146          0         0   \n",
            "1314287950658760709        TheLALook        336          0         0   \n",
            "1314286729877094400          gojaxgo        186          0         0   \n",
            "1314286728220413952    PandaMonyToys        444          2         2   \n",
            "1314286090128297984         laxradar        158          0         0   \n",
            "1314285710652788736         laxradar        158          0         0   \n",
            "1314285223564115968         laxradar        158          0         0   \n",
            "1314284585379745793        lngglobal      24807          2         0   \n",
            "1314283772934717440         laxradar        158          0         0   \n",
            "1314281017906143233      Pteamtrades         18          0         0   \n",
            "\n",
            "                     tb_sentiment                query_term  \\\n",
            "id                                                            \n",
            "1314325575608098816            -1  \"delta\" -filter:retweets   \n",
            "1314325180148113408             1  \"delta\" -filter:retweets   \n",
            "1314324111246594049            -1  \"delta\" -filter:retweets   \n",
            "1314323830072987649             0  \"delta\" -filter:retweets   \n",
            "1314323312013725698            -1  \"delta\" -filter:retweets   \n",
            "1314317343279865856             1  \"delta\" -filter:retweets   \n",
            "1314316153150271488             0  \"delta\" -filter:retweets   \n",
            "1314314245316595713            -1  \"delta\" -filter:retweets   \n",
            "1314313662237032448             0  \"delta\" -filter:retweets   \n",
            "1314313393830916096             0  \"delta\" -filter:retweets   \n",
            "1314313253468561408             0  \"delta\" -filter:retweets   \n",
            "1314312107236573184             0  \"delta\" -filter:retweets   \n",
            "1314311089748762624             0  \"delta\" -filter:retweets   \n",
            "1314310985671344128             0  \"delta\" -filter:retweets   \n",
            "1314310456102715393             1  \"delta\" -filter:retweets   \n",
            "1314309403319836673             0  \"delta\" -filter:retweets   \n",
            "1314308962389377025            -1  \"delta\" -filter:retweets   \n",
            "1314308461690122240             0  \"delta\" -filter:retweets   \n",
            "1314308251068948480             0  \"delta\" -filter:retweets   \n",
            "1314308209142718464             1  \"delta\" -filter:retweets   \n",
            "1314306591781392385             0  \"delta\" -filter:retweets   \n",
            "1314306457630695424             0  \"delta\" -filter:retweets   \n",
            "1314304812339523584             0  \"delta\" -filter:retweets   \n",
            "1314304583011835920             0  \"delta\" -filter:retweets   \n",
            "1314304413138329608            -1  \"delta\" -filter:retweets   \n",
            "1314304122951094273             0  \"delta\" -filter:retweets   \n",
            "1314301051470704645             0  \"delta\" -filter:retweets   \n",
            "1314300291240525825             0  \"delta\" -filter:retweets   \n",
            "1314300157144440833             0  \"delta\" -filter:retweets   \n",
            "1314299231834759168             0  \"delta\" -filter:retweets   \n",
            "1314297942715621377             0  \"delta\" -filter:retweets   \n",
            "1314297075228499969             1  \"delta\" -filter:retweets   \n",
            "1314296791270006784             0  \"delta\" -filter:retweets   \n",
            "1314296714740690944             1  \"delta\" -filter:retweets   \n",
            "1314296551888621568             1  \"delta\" -filter:retweets   \n",
            "1314295715917570048             1  \"delta\" -filter:retweets   \n",
            "1314293141923487744             0  \"delta\" -filter:retweets   \n",
            "1314291775608160257             0  \"delta\" -filter:retweets   \n",
            "1314290127846342657             0  \"delta\" -filter:retweets   \n",
            "1314289130386255872             0  \"delta\" -filter:retweets   \n",
            "1314288139884589056             0  \"delta\" -filter:retweets   \n",
            "1314287950658760709             0  \"delta\" -filter:retweets   \n",
            "1314286729877094400            -1  \"delta\" -filter:retweets   \n",
            "1314286728220413952             1  \"delta\" -filter:retweets   \n",
            "1314286090128297984             0  \"delta\" -filter:retweets   \n",
            "1314285710652788736             0  \"delta\" -filter:retweets   \n",
            "1314285223564115968             0  \"delta\" -filter:retweets   \n",
            "1314284585379745793             0  \"delta\" -filter:retweets   \n",
            "1314283772934717440             0  \"delta\" -filter:retweets   \n",
            "1314281017906143233            -1  \"delta\" -filter:retweets   \n",
            "\n",
            "                             scrape_time  \n",
            "id                                        \n",
            "1314325575608098816  10/08/2020 15:04:26  \n",
            "1314325180148113408  10/08/2020 15:04:26  \n",
            "1314324111246594049  10/08/2020 15:04:26  \n",
            "1314323830072987649  10/08/2020 15:04:26  \n",
            "1314323312013725698  10/08/2020 15:04:26  \n",
            "1314317343279865856  10/08/2020 15:04:26  \n",
            "1314316153150271488  10/08/2020 15:04:26  \n",
            "1314314245316595713  10/08/2020 15:04:26  \n",
            "1314313662237032448  10/08/2020 15:04:26  \n",
            "1314313393830916096  10/08/2020 15:04:26  \n",
            "1314313253468561408  10/08/2020 15:04:26  \n",
            "1314312107236573184  10/08/2020 15:04:26  \n",
            "1314311089748762624  10/08/2020 15:04:26  \n",
            "1314310985671344128  10/08/2020 15:04:26  \n",
            "1314310456102715393  10/08/2020 15:04:26  \n",
            "1314309403319836673  10/08/2020 15:04:26  \n",
            "1314308962389377025  10/08/2020 15:04:26  \n",
            "1314308461690122240  10/08/2020 15:04:26  \n",
            "1314308251068948480  10/08/2020 15:04:26  \n",
            "1314308209142718464  10/08/2020 15:04:26  \n",
            "1314306591781392385  10/08/2020 15:04:26  \n",
            "1314306457630695424  10/08/2020 15:04:26  \n",
            "1314304812339523584  10/08/2020 15:04:26  \n",
            "1314304583011835920  10/08/2020 15:04:26  \n",
            "1314304413138329608  10/08/2020 15:04:26  \n",
            "1314304122951094273  10/08/2020 15:04:26  \n",
            "1314301051470704645  10/08/2020 15:04:26  \n",
            "1314300291240525825  10/08/2020 15:04:26  \n",
            "1314300157144440833  10/08/2020 15:04:26  \n",
            "1314299231834759168  10/08/2020 15:04:26  \n",
            "1314297942715621377  10/08/2020 15:04:26  \n",
            "1314297075228499969  10/08/2020 15:04:26  \n",
            "1314296791270006784  10/08/2020 15:04:26  \n",
            "1314296714740690944  10/08/2020 15:04:26  \n",
            "1314296551888621568  10/08/2020 15:04:26  \n",
            "1314295715917570048  10/08/2020 15:04:26  \n",
            "1314293141923487744  10/08/2020 15:04:26  \n",
            "1314291775608160257  10/08/2020 15:04:26  \n",
            "1314290127846342657  10/08/2020 15:04:26  \n",
            "1314289130386255872  10/08/2020 15:04:26  \n",
            "1314288139884589056  10/08/2020 15:04:26  \n",
            "1314287950658760709  10/08/2020 15:04:26  \n",
            "1314286729877094400  10/08/2020 15:04:26  \n",
            "1314286728220413952  10/08/2020 15:04:26  \n",
            "1314286090128297984  10/08/2020 15:04:26  \n",
            "1314285710652788736  10/08/2020 15:04:26  \n",
            "1314285223564115968  10/08/2020 15:04:26  \n",
            "1314284585379745793  10/08/2020 15:04:26  \n",
            "1314283772934717440  10/08/2020 15:04:26  \n",
            "1314281017906143233  10/08/2020 15:04:26  >\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Bth8Otnxw6"
      },
      "source": [
        "**Sample search results:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TuS8Kill5L5"
      },
      "source": [
        "                                                                tweets  \\\n",
        "\n",
        "id                                                                       \n",
        "1313596177841975296  Human Rights Watch is against CA Prop 25 (2020...   \n",
        "1313535404172247041  Prop 25 is very contentious. I'm going to dig ...   \n",
        "1313382184024043521  @CoCoSouthLA @LAP\n",
        "aysAttention \"'with Propositi...   \n",
        "1313381072760041472  @CoCoSouthLA @LAPaysAttention Prop 25 essentia...   \n",
        "1313305141609549824  Very important read. I trust HRW on all things...   \n",
        "\n",
        "\n",
        "                                    date  len latlon                user_loc  \\\n",
        "id                                                                             \n",
        "1313596177841975296  10/06/2020 21:45:08   96   None         Los Angeles, CA   \n",
        "1313535404172247041  10/06/2020 17:43:39  133   None        Venice Beach, CA   \n",
        "1313382184024043521  10/06/2020 07:34:48  139   None         Los Angeles, CA   \n",
        "1313381072760041472  10/06/2020 07:30:23  140   None         Los Angeles, CA   \n",
        "1313305141609549824  10/06/2020 02:28:40   97   None             Los Angeles   \n",
        "   \n",
        "\n",
        "                         user_handle  followers  favorites  retweets  \\\n",
        "id                                                                     \n",
        "1313596177841975296  CalvinStarnesOG       4039          0         0   \n",
        "1313535404172247041      antifa_chad       1541          2         0   \n",
        "1313382184024043521      its_a_lotte        912          0         0   \n",
        "1313381072760041472      its_a_lotte        912          0         0   \n",
        "1313305141609549824     Benjaminlear        869          3         0   \n",
        "\n",
        "\n",
        "                     tb_sentiment                         query_term  \\\n",
        "id                                                                     \n",
        "1313596177841975296             0  \"proposition 25\" -filter:retweets   \n",
        "1313535404172247041             1  \"proposition 25\" -filter:retweets   \n",
        "1313382184024043521             0  \"proposition 25\" -filter:retweets   \n",
        "1313381072760041472             0  \"proposition 25\" -filter:retweets   \n",
        "1313305141609549824             1  \"proposition 25\" -filter:retweets   \n",
        "\n",
        "\n",
        "                             scrape_time  \n",
        "id                                        \n",
        "1313596177841975296  10/06/2020 17:44:53  \n",
        "1313535404172247041  10/06/2020 17:44:53  \n",
        "1313382184024043521  10/06/2020 17:44:53  \n",
        "1313381072760041472  10/06/2020 17:44:53  \n",
        "1313305141609549824  10/06/2020 17:44:53    >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHL_LcA3lHSG"
      },
      "source": [
        "### 1.2: Scraping Tweets and saving data to .csv\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEhLw0ASlXc9"
      },
      "source": [
        "# only run this line of code if you are creating a new file for your new search.\n",
        "# running this twice after creating a new csv will replace the information you assigned the first time\n",
        "\n",
        "#df.to_csv('../data/ca_prop_tweets.csv', index='True', index_label='id', encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAunc4bAkxKo"
      },
      "source": [
        "# here we are adding new search results to the .csv file we created. Thanks pandas!\n",
        "# we are dropping duplicate records in case our subsequent searches garner the same tweets\n",
        "\n",
        "def append_new_tweets(master_file, new_tweets):\n",
        "  master_file = pd.concat([master_file, new_tweets])\n",
        "  master_file = master_file.drop_duplicates(subset=['date', 'tweets'], keep='first')\n",
        "  master_file.to_csv('ca_prop_tweets.csv',\n",
        "                     index=True,\n",
        "                     index_label='id',\n",
        "                     encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zje3xhnYlFA5"
      },
      "source": [
        "master_file = pd.read_csv('ca_prop_tweets.csv', index_col='id')\n",
        "\n",
        "append_new_tweets(master_file, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keC-pQSer801",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "outputId": "27b3a5d8-2169-4f5f-b74b-b52f608a82c6"
      },
      "source": [
        "# and voila! Let's take a look at our cleaned up .csv dataframe in the next section\n",
        "\n",
        "master_file"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                tweets  \\\n",
              "id                                                                       \n",
              "1313626138246168577  @p1rat3girl08 Thanks for the input! So just to...   \n",
              "1313625012310429696  https://t.co/8QN3R7Jo8Z @KNOCKdotLA have a gre...   \n",
              "1313624353523736576  @struthioniforme @mattyglesias Isn't Prop 22 a...   \n",
              "1313619501980635136  just a reminder, if prop 22 PASSES, it will ta...   \n",
              "1313617735113306112  If any other Uber/Lyft engineers want to come ...   \n",
              "...                                                                ...   \n",
              "1311831516343795712  @MinorityJustNow @JusticeLANow Human Rights Wa...   \n",
              "1311831196704231425  @CarboneLukas Like @isaacscher mentioned below...   \n",
              "1311410931751030785  @stopprop25 @banales_adrian Actually, Proposit...   \n",
              "1311383565192572929  Endorsement: Yes on Proposition 25 to end bail...   \n",
              "1311247672590954496  Endorsement: Yes on Proposition 25 to end bail...   \n",
              "\n",
              "                                    date  len  latlon  \\\n",
              "id                                                      \n",
              "1313626138246168577  10/06/2020 23:44:12  118     NaN   \n",
              "1313625012310429696  10/06/2020 23:39:43  140     NaN   \n",
              "1313624353523736576  10/06/2020 23:37:06   83     NaN   \n",
              "1313619501980635136  10/06/2020 23:17:49  139     NaN   \n",
              "1313617735113306112  10/06/2020 23:10:48  140     NaN   \n",
              "...                                  ...  ...     ...   \n",
              "1311831516343795712  10/02/2020 00:53:00  140     NaN   \n",
              "1311831196704231425  10/02/2020 00:51:44  140     NaN   \n",
              "1311410931751030785  09/30/2020 21:01:45  140     NaN   \n",
              "1311383565192572929  09/30/2020 19:13:00   94     NaN   \n",
              "1311247672590954496  09/30/2020 10:13:01  116     NaN   \n",
              "\n",
              "                                    user_loc     user_handle  followers  \\\n",
              "id                                                                        \n",
              "1313626138246168577          Los Angeles, CA    thealdywaldy       3528   \n",
              "1313625012310429696  Los Angeles, California     theduncanbo        717   \n",
              "1313624353523736576             Pasadena, CA        VATVSLPR        183   \n",
              "1313619501980635136          Los Angeles, CA      katiemcvay       4828   \n",
              "1313617735113306112                    SF/LA        alicec47       1005   \n",
              "...                                      ...             ...        ...   \n",
              "1311831516343795712              Los Angeles     MamaSiobhan        188   \n",
              "1311831196704231425              Los Angeles     MamaSiobhan        188   \n",
              "1311410931751030785          Los Angeles, CA     pjrodriguez        884   \n",
              "1311383565192572929           El Segundo, CA  latimesopinion      20311   \n",
              "1311247672590954496           El Segundo, CA         latimes    3687140   \n",
              "\n",
              "                     favorites  retweets  tb_sentiment  \\\n",
              "id                                                       \n",
              "1313626138246168577          0         0             1   \n",
              "1313625012310429696          0         0             1   \n",
              "1313624353523736576          0         0             0   \n",
              "1313619501980635136          8         0             0   \n",
              "1313617735113306112         11         2            -1   \n",
              "...                        ...       ...           ...   \n",
              "1311831516343795712          1         0            -1   \n",
              "1311831196704231425          0         0             0   \n",
              "1311410931751030785          0         0             1   \n",
              "1311383565192572929          1         1             0   \n",
              "1311247672590954496         39        20             0   \n",
              "\n",
              "                                            query_term          scrape_time  \n",
              "id                                                                           \n",
              "1313626138246168577         \"prop 22\" -filter:retweets  10/06/2020 16:45:30  \n",
              "1313625012310429696         \"prop 22\" -filter:retweets  10/06/2020 16:45:30  \n",
              "1313624353523736576         \"prop 22\" -filter:retweets  10/06/2020 16:45:30  \n",
              "1313619501980635136         \"prop 22\" -filter:retweets  10/06/2020 16:45:30  \n",
              "1313617735113306112         \"prop 22\" -filter:retweets  10/06/2020 16:45:30  \n",
              "...                                                ...                  ...  \n",
              "1311831516343795712  \"proposition 25\" -filter:retweets  10/06/2020 17:44:53  \n",
              "1311831196704231425  \"proposition 25\" -filter:retweets  10/06/2020 17:44:53  \n",
              "1311410931751030785  \"proposition 25\" -filter:retweets  10/06/2020 17:44:53  \n",
              "1311383565192572929  \"proposition 25\" -filter:retweets  10/06/2020 17:44:53  \n",
              "1311247672590954496  \"proposition 25\" -filter:retweets  10/06/2020 17:44:53  \n",
              "\n",
              "[1542 rows x 12 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweets</th>\n",
              "      <th>date</th>\n",
              "      <th>len</th>\n",
              "      <th>latlon</th>\n",
              "      <th>user_loc</th>\n",
              "      <th>user_handle</th>\n",
              "      <th>followers</th>\n",
              "      <th>favorites</th>\n",
              "      <th>retweets</th>\n",
              "      <th>tb_sentiment</th>\n",
              "      <th>query_term</th>\n",
              "      <th>scrape_time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1313626138246168577</th>\n",
              "      <td>@p1rat3girl08 Thanks for the input! So just to...</td>\n",
              "      <td>10/06/2020 23:44:12</td>\n",
              "      <td>118</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Los Angeles, CA</td>\n",
              "      <td>thealdywaldy</td>\n",
              "      <td>3528</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>\"prop 22\" -filter:retweets</td>\n",
              "      <td>10/06/2020 16:45:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1313625012310429696</th>\n",
              "      <td>https://t.co/8QN3R7Jo8Z @KNOCKdotLA have a gre...</td>\n",
              "      <td>10/06/2020 23:39:43</td>\n",
              "      <td>140</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Los Angeles, California</td>\n",
              "      <td>theduncanbo</td>\n",
              "      <td>717</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>\"prop 22\" -filter:retweets</td>\n",
              "      <td>10/06/2020 16:45:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1313624353523736576</th>\n",
              "      <td>@struthioniforme @mattyglesias Isn't Prop 22 a...</td>\n",
              "      <td>10/06/2020 23:37:06</td>\n",
              "      <td>83</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pasadena, CA</td>\n",
              "      <td>VATVSLPR</td>\n",
              "      <td>183</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>\"prop 22\" -filter:retweets</td>\n",
              "      <td>10/06/2020 16:45:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1313619501980635136</th>\n",
              "      <td>just a reminder, if prop 22 PASSES, it will ta...</td>\n",
              "      <td>10/06/2020 23:17:49</td>\n",
              "      <td>139</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Los Angeles, CA</td>\n",
              "      <td>katiemcvay</td>\n",
              "      <td>4828</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>\"prop 22\" -filter:retweets</td>\n",
              "      <td>10/06/2020 16:45:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1313617735113306112</th>\n",
              "      <td>If any other Uber/Lyft engineers want to come ...</td>\n",
              "      <td>10/06/2020 23:10:48</td>\n",
              "      <td>140</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SF/LA</td>\n",
              "      <td>alicec47</td>\n",
              "      <td>1005</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>-1</td>\n",
              "      <td>\"prop 22\" -filter:retweets</td>\n",
              "      <td>10/06/2020 16:45:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1311831516343795712</th>\n",
              "      <td>@MinorityJustNow @JusticeLANow Human Rights Wa...</td>\n",
              "      <td>10/02/2020 00:53:00</td>\n",
              "      <td>140</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Los Angeles</td>\n",
              "      <td>MamaSiobhan</td>\n",
              "      <td>188</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>\"proposition 25\" -filter:retweets</td>\n",
              "      <td>10/06/2020 17:44:53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1311831196704231425</th>\n",
              "      <td>@CarboneLukas Like @isaacscher mentioned below...</td>\n",
              "      <td>10/02/2020 00:51:44</td>\n",
              "      <td>140</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Los Angeles</td>\n",
              "      <td>MamaSiobhan</td>\n",
              "      <td>188</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>\"proposition 25\" -filter:retweets</td>\n",
              "      <td>10/06/2020 17:44:53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1311410931751030785</th>\n",
              "      <td>@stopprop25 @banales_adrian Actually, Proposit...</td>\n",
              "      <td>09/30/2020 21:01:45</td>\n",
              "      <td>140</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Los Angeles, CA</td>\n",
              "      <td>pjrodriguez</td>\n",
              "      <td>884</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>\"proposition 25\" -filter:retweets</td>\n",
              "      <td>10/06/2020 17:44:53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1311383565192572929</th>\n",
              "      <td>Endorsement: Yes on Proposition 25 to end bail...</td>\n",
              "      <td>09/30/2020 19:13:00</td>\n",
              "      <td>94</td>\n",
              "      <td>NaN</td>\n",
              "      <td>El Segundo, CA</td>\n",
              "      <td>latimesopinion</td>\n",
              "      <td>20311</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>\"proposition 25\" -filter:retweets</td>\n",
              "      <td>10/06/2020 17:44:53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1311247672590954496</th>\n",
              "      <td>Endorsement: Yes on Proposition 25 to end bail...</td>\n",
              "      <td>09/30/2020 10:13:01</td>\n",
              "      <td>116</td>\n",
              "      <td>NaN</td>\n",
              "      <td>El Segundo, CA</td>\n",
              "      <td>latimes</td>\n",
              "      <td>3687140</td>\n",
              "      <td>39</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>\"proposition 25\" -filter:retweets</td>\n",
              "      <td>10/06/2020 17:44:53</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1542 rows Ã— 12 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTd_E2T4mzCy"
      },
      "source": [
        "## 2: Examining & manipulating unstructured text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAQfd7hholTi"
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import re\n",
        "import seaborn as sns\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
        "from nltk.classify import ClassifierI\n",
        "from statistics import mode\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msOsJYaAld-Z"
      },
      "source": [
        "# let's look at our gorgeous dataframe...\n",
        "# take 1 minute and see if anything strikes you...\n",
        "\n",
        "df = pd.read_csv('../data/ca_prop_tweets.csv', index_col='id').sample(frac = 1)\n",
        "\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1g2Y81Ix3uv"
      },
      "source": [
        "# look familiar?\n",
        "\n",
        "type(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM0y79Ad3x-e"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-LIdXpqyFh2"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxlUc_ujqain"
      },
      "source": [
        "df['tweets'].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkWWib_23MJW"
      },
      "source": [
        "# let's look at one more column and expand our visibility to see more of our tweets...\n",
        "# take two minutes and consider what the textblob sentiment analysis is capturing (1 = positive, -1 = negative, 0 = neutral)\n",
        "\n",
        "pd.options.display.max_colwidth = 300\n",
        "\n",
        "df[['tweets', 'tb_sentiment']].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcR7ntpB46vV"
      },
      "source": [
        "One thing to consider: is my sentiment analysis providing meaningful information regarding my research question?\n",
        "\n",
        "<br>\n",
        "\n",
        "What kind of research questions can we address with this data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNe2yiHoLvAM"
      },
      "source": [
        "# How many occurances of no and yes are in our tweets?\n",
        "\n",
        "for t in df['tweets'][:10]:\n",
        "  print(t.lower().count(\"no\"), \"mention(s) of NO and\", t.lower().count(\"yes\"), \"mention(s) of YES\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8ZFKL4-LpRJ"
      },
      "source": [
        "# let's create two variables to explore trends in support/opposition\n",
        "# notice anything fishy? You might or you might not...\n",
        "\n",
        "df['num_yes'] = df['tweets'].str.lower().str.count(\"yes\")\n",
        "df['num_no'] = df['tweets'].str.lower().str.count('no')\n",
        "\n",
        "df[['num_yes', 'num_no', 'tweets']].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfY20dEy7JDF"
      },
      "source": [
        "At this stage, we need to make a few assumptions about our data to begin to examine our RQ...those assumptions might be:\n",
        "\n",
        "\n",
        "*   records with more than 3 instances of NO and/or YES are likely voter guides\n",
        "*   records with 0 instances of NO AND YES will require further examination (that we do not currently have time for). We can drop these cases.\n",
        "*   records with equivalent NO and YES counts will also require further investigation and can be dropped for right now.\n",
        "*   records with a higher NO count than YES count likely signal opposition to the proposition, and vice versa (this is not always the case and further investigation is required before we can make this kind of assumption)\n",
        "*   Any others?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCeNvEV_Rt0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9877e867-2ed9-487a-a996-1ebfcbc46003"
      },
      "source": [
        "# Filtering out guides...\n",
        "\n",
        "mask_not_guide = (df['num_no'] < 4) & (df['num_yes'] <4) & ((df['num_no'] + df['num_yes']) <4)\n",
        "\n",
        "# Filtering out cases that do not state support or opposition with YES/NO\n",
        "\n",
        "mask_clear_position = (df['num_no'] > 0) | (df['num_yes'] > 0)\n",
        "\n",
        "# Filtering out cases with no clear position\n",
        "\n",
        "mask_unclear_position = (df['num_no'] == df['num_yes'])\n",
        "\n",
        "mask = mask_not_guide & mask_clear_position & ~mask_unclear_position\n",
        "\n",
        "df[mask].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(707, 14)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKN4LdcR-tMb"
      },
      "source": [
        "# any better?\n",
        "\n",
        "df_new = df[mask]\n",
        "df_new.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddfxYzOWAh09"
      },
      "source": [
        "# let's create a new variable, \"user_stance\", to ID supporters and opposers based on our criteria\n",
        "\n",
        "pd.set_option('mode.chained.assignment', None)\n",
        "\n",
        "mask_support = df_new['num_no'] < df_new['num_yes']\n",
        "\n",
        "df_new.loc[mask_support, 'user_stance'] = \"Support\"\n",
        "df_new.loc[~mask_support, 'user_stance'] = \"Oppose\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFISATlIHMoK"
      },
      "source": [
        "df_new[['tweets', 'user_stance']].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhGh4SA0Hd_U"
      },
      "source": [
        "##3: Visualizing trends from content analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ6yHr9QHrZ9"
      },
      "source": [
        "# aaaaand one more piece of housekeeping: let's create a field to identify our propositions\n",
        "# pandas is loop averse...if anyone has any ideas for doing this operation more efficiently...please.\n",
        "\n",
        "mask_14 = df_new['query_term'].str.contains('14') == True\n",
        "mask_15 = df_new['query_term'].str.contains('15') == True\n",
        "mask_16 = df_new['query_term'].str.contains('16') == True\n",
        "mask_17 = df_new['query_term'].str.contains('17') == True\n",
        "mask_18 = df_new['query_term'].str.contains('18') == True\n",
        "mask_19 = df_new['query_term'].str.contains('19') == True\n",
        "mask_20 = df_new['query_term'].str.contains('20') == True\n",
        "mask_21 = df_new['query_term'].str.contains('21') == True\n",
        "mask_22 = df_new['query_term'].str.contains('22') == True\n",
        "mask_23 = df_new['query_term'].str.contains('23') == True\n",
        "mask_24 = df_new['query_term'].str.contains('24') == True\n",
        "mask_25 = df_new['query_term'].str.contains('25') == True\n",
        "\n",
        "df_new.loc[mask_14, 'proposition'] = \"Proposition 14\"\n",
        "df_new.loc[mask_15, 'proposition'] = \"Proposition 15\"\n",
        "df_new.loc[mask_16, 'proposition'] = \"Proposition 16\"\n",
        "df_new.loc[mask_17, 'proposition'] = \"Proposition 17\"\n",
        "df_new.loc[mask_18, 'proposition'] = \"Proposition 18\"\n",
        "df_new.loc[mask_19, 'proposition'] = \"Proposition 19\"\n",
        "df_new.loc[mask_20, 'proposition'] = \"Proposition 20\"\n",
        "df_new.loc[mask_21, 'proposition'] = \"Proposition 21\"\n",
        "df_new.loc[mask_22, 'proposition'] = \"Proposition 22\"\n",
        "df_new.loc[mask_23, 'proposition'] = \"Proposition 23\"\n",
        "df_new.loc[mask_24, 'proposition'] = \"Proposition 24\"\n",
        "df_new.loc[mask_25, 'proposition'] = \"Proposition 25\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb-b2W9NWWxZ"
      },
      "source": [
        "# nice...\n",
        "\n",
        "df_new[['tweets', 'user_stance', 'proposition']].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTyDkIq_WiTS"
      },
      "source": [
        "# so how are Twitter users speaking about the CA props?\n",
        "\n",
        "df_new = df_new.sort_values(by=['proposition'])\n",
        "\n",
        "ax = sns.countplot(x=df_new['proposition'],\n",
        "                   hue=df_new['user_stance'],\n",
        "                   alpha=0.8)\n",
        "ax.set_xticklabels(ax.get_xticklabels(),\n",
        "                   rotation=45,\n",
        "                   horizontalalignment='right')\n",
        "ax.set_xlabel('Nov 2020 California Propositions')\n",
        "ax.set_ylabel('Number of Unique Tweets')\n",
        "ax.set_title('Twitter User Stances on CA Props (October 2020)')\n",
        "\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "ax.legend(handles=handles[2:], labels=labels[2:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5C0KsLtjUZd"
      },
      "source": [
        "# Are influencers behaving any differently on twitter?\n",
        "\n",
        "mask_influencer = df_new['followers'] >=10000\n",
        "df_influencer = df_new[mask_influencer]\n",
        "\n",
        "ax = sns.countplot(x=df_influencer['proposition'],\n",
        "                   hue=df_influencer['user_stance'],\n",
        "                   alpha=0.8)\n",
        "ax.set_xticklabels(ax.get_xticklabels(),\n",
        "                   rotation=45,\n",
        "                   horizontalalignment='right')\n",
        "ax.set_xlabel('Nov 2020 California Propositions')\n",
        "ax.set_ylabel('Number of Unique Tweets')\n",
        "ax.set_title('INFLUENCER Stances on CA Props (October 2020)')\n",
        "\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "ax.legend(handles=handles[2:], labels=labels[2:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T53Wn7a4Jtl"
      },
      "source": [
        "# PART B: Basic Sentiment Analysis with Movie Reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFXKlDMonf0U"
      },
      "source": [
        "Now let's move on to another form of natural language processing using machine learning. In this section, we recreate from scratch a process similar to the textblob NLP method we applied to our Tweets earlier in this notebook. Together we will train an algorithm to identify positive and negative sentiment from ANY text using movie review data. In theory, you could use the algorithm we are training to study polarity in any text...but should you?\n",
        "\n",
        "<br>\n",
        "\n",
        "There are many different approaches to sentiment analysis we might take, but for today, let's explore a basic categorical approach that treats of texts as bags of words (BOW). A BOW approach to sentiment analysis focuses on sentimental value from the individual words in our texts, but ignores advanced information such as sarcasm or grammar. For simplicity's sake, we will be using a popular positive/negative sentiment lexicon developed by Hu & liu (2004) to compare with our movie reviews and to train our algorithm!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFmJdcM74VSu"
      },
      "source": [
        "## 4: Cleaning Text Data for Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGE2BGo84k3E"
      },
      "source": [
        "# let's ask nltk to download a few important files to our current folder\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVzYuksN4tbj"
      },
      "source": [
        "pos_folder = os.listdir('../data/train/pos')\n",
        "neg_folder = os.listdir('../data/train/neg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdPriVZM42om"
      },
      "source": [
        "print(len(pos_folder))\n",
        "print(len(neg_folder))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2Sxgve-43UD"
      },
      "source": [
        "print(pos_folder[0:5])\n",
        "print(neg_folder[0:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xusJzvzS45ts"
      },
      "source": [
        "type(neg_folder[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQvIlCTO4760"
      },
      "source": [
        "# You can open your .txt files in python to examine sentence structure, punctuation issues, etc.\n",
        "# These text files are pretty clean! We still have some grooming to do before we can use them to model...\n",
        "\n",
        "positive_review = open('../data/train/pos/'+pos_folder[0], 'r').read()\n",
        "negative_review = open('../data/train/neg/'+neg_folder[0], 'r').read()\n",
        "\n",
        "print('POSITIVE REVIEW:', positive_review)\n",
        "print('NEGATIVE REVIEW:', negative_review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEdw8Bo75BDE"
      },
      "source": [
        "# you can also shuffle through folders using the random package\n",
        "# you might use this method to acquaint yourself with large datasets like these\n",
        "\n",
        "random.shuffle(pos_folder)\n",
        "random.shuffle(neg_folder)\n",
        "\n",
        "positive_review = open('../data/train/pos/'+pos_folder[0], 'r').read()\n",
        "negative_review = open('../data/train/neg/'+neg_folder[0], 'r').read()\n",
        "\n",
        "print('RANDOM POSITIVE REVIEW:', positive_review)\n",
        "print('RANDOM NEGATIVE REVIEW:', negative_review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44wA0CNL5Elz"
      },
      "source": [
        "# let's take random samples from the negative and positive review folders to save processing time later on\n",
        "\n",
        "random.shuffle(pos_folder)\n",
        "pos_folder = pos_folder[:1500]\n",
        "\n",
        "random.shuffle(neg_folder)\n",
        "neg_folder = neg_folder[:1500]\n",
        "\n",
        "print(len(pos_folder))\n",
        "print(len(neg_folder))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zBv8X4X5UP5"
      },
      "source": [
        "# now let's create a list containing all of our opened movie review files...\n",
        "# by appending the .txt. file name to a path string\n",
        "\n",
        "files_positive = []\n",
        "files_negative = []\n",
        "\n",
        "for file in pos_folder:\n",
        "  files_positive.append(open('../data/train/pos/'+file, 'r').read())\n",
        "\n",
        "for file in neg_folder:\n",
        "  files_negative.append(open('../data/train/neg/'+file, 'r').read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmHcex835Vwi"
      },
      "source": [
        "# aaaand here is a sample of our list\n",
        "\n",
        "print(files_positive[0])\n",
        "print(files_positive[1])\n",
        "print(files_positive[0:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRo51y3W5ZSU"
      },
      "source": [
        "# removing all non-alphabetical content allows us to tokenize words if that is part of our NLP process.\n",
        "# here we are using something called a regular expression to clean our reviews...\n",
        "# this is an advanced topic, so do not stress the syntax\n",
        "\n",
        "no_punctuation = []\n",
        "\n",
        "for review in files_positive:\n",
        "  no_punctuation.append(re.sub(r'[^a-zA-Z\\s]','', review))\n",
        "\n",
        "no_punctuation[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgAdMFsJ5fkI"
      },
      "source": [
        "# and here we transformed all upper-case letters to lower-case letters\n",
        "\n",
        "pos_cleaned = []\n",
        "\n",
        "for string in no_punctuation:\n",
        "  pos_cleaned.append(string.lower())\n",
        "\n",
        "pos_cleaned[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEjSVtj95i--"
      },
      "source": [
        "# ditto for our negative reviews\n",
        "\n",
        "neg_cleaned = []\n",
        "no_punct = []\n",
        "\n",
        "for review in files_negative:\n",
        "  neg_cleaned.append(re.sub(r'[^a-zA-Z\\s]','',review.lower()))\n",
        "\n",
        "neg_cleaned[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "san_RbTKI2-D"
      },
      "source": [
        "all_cleaned = pos_cleaned + neg_cleaned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-0E3eFT5spu"
      },
      "source": [
        "## 5: Training, Validation, & Test Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QV05uwj6jNl"
      },
      "source": [
        "5.1: Cleaning, tokenizing, and creating a lexicon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9ogH5Lt5rCU"
      },
      "source": [
        "# Tokenizers are used to split strings into lists of substrings\n",
        "# word_tokenize() from the nltk package divides strings at punctuation marks other than periods.\n",
        "\n",
        "tokenized = []\n",
        "\n",
        "for review in pos_cleaned:\n",
        "    review_tokens = word_tokenize(review)\n",
        "    for word in review_tokens:\n",
        "      tokenized.append(word)\n",
        "\n",
        "print('There are', len(tokenized), 'words in my batch of positive reviews')\n",
        "print(tokenized[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgsJqMoa6EkY"
      },
      "source": [
        "# stop words are common 'empty' words that we can filter out to create space for words/phrases with sentimental weight\n",
        "# here we are using the stopwords constant that we downloaded earlier from NLTK, but you can create your own as well\n",
        "\n",
        "stop_words = list(set(stopwords.words('english')))\n",
        "sorted(stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d03xGTKp6AYi"
      },
      "source": [
        "no_stops = []\n",
        "\n",
        "for word in tokenized:\n",
        "    if word not in stop_words:\n",
        "      no_stops.append(word)\n",
        "\n",
        "print('I removed', (len(tokenized)-len(no_stops)), 'stop words!!')\n",
        "print(no_stops[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhqH3wGL6YDN"
      },
      "source": [
        "# here we are asking nltk to tag all of our tokenized words with a semantic part of speech identifier\n",
        "\n",
        "part_of_speech_positive = nltk.pos_tag(no_stops)\n",
        "\n",
        "part_of_speech_positive[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWa6zyJT6b_5"
      },
      "source": [
        "# allowed_word_types = [\"J\",\"R\",\"V\"]\n",
        "# J = adjectives, R = adverbs, and V = verbs\n",
        "\n",
        "adjectives = [\"J\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urCFO4_N6d63"
      },
      "source": [
        "all_pos_adjectives = []\n",
        "\n",
        "for word in part_of_speech_positive:\n",
        "  if word[1][0] in adjectives:\n",
        "    all_pos_adjectives.append(word[0])\n",
        "\n",
        "all_pos_adjectives[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur6a6HtU6ffI"
      },
      "source": [
        "all_neg_adjectives = []\n",
        "tokenizedb = []\n",
        "no_stopsb = []\n",
        "\n",
        "for review in neg_cleaned:\n",
        "    review_tokens = word_tokenize(review)\n",
        "    for word in review_tokens:\n",
        "      tokenizedb.append(word)\n",
        "\n",
        "\n",
        "for word in tokenizedb:\n",
        "    if word not in stop_words:\n",
        "      no_stopsb.append(word)\n",
        "\n",
        "part_of_speech_negative = nltk.pos_tag(no_stopsb)\n",
        "\n",
        "for word in part_of_speech_negative:\n",
        "  if word[1][0] in adjectives:\n",
        "    all_neg_adjectives.append(word[0])\n",
        "\n",
        "all_neg_adjectives[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6600ONcxgu3"
      },
      "source": [
        "# and voila!\n",
        "\n",
        "lex = all_pos_adjectives + all_pos_adjectives"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACMQ1gd96sJf"
      },
      "source": [
        "4.2: Constructing your feature sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0zh5kLe2rPe"
      },
      "source": [
        "# now onto the cool stuff...to begin our analysis, let's create a list of tuples with\n",
        "# our text information and pos/neg attributes on either side of each pair.\n",
        "\n",
        "documents = []\n",
        "\n",
        "for review in files_positive:\n",
        "  pos_docs = re.sub(r'[^a-zA-Z\\s]', '',review)\n",
        "  documents.append((pos_docs.lower(), \"pos\"))\n",
        "\n",
        "for review in files_negative:\n",
        "  neg_docs = re.sub(r'[^a-zA-Z\\s]', '',review)\n",
        "  documents.append((neg_docs.lower(), \"neg\"))\n",
        "\n",
        "documents[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_N6JaKTr61_s"
      },
      "source": [
        "# in another module, you might use the lexicon we developed earlier, ID'ed the\n",
        "# most common words and used those strings to train our algorithm.\n",
        "# let's instead use a tried and true sentiment lexicon...now we are not limited to adjectives\n",
        "\n",
        "pos_file = open('../data/lexicon/pos.txt', 'r').read()\n",
        "neg_file = open('../data/lexicon/neg.txt', encoding = 'ISO=8859-1').read()\n",
        "\n",
        "pos_words = word_tokenize(pos_file)\n",
        "random.shuffle(pos_words)\n",
        "neg_words = word_tokenize(neg_file)\n",
        "random.shuffle(neg_words)\n",
        "\n",
        "all_sent_words = pos_words + neg_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANjduYNy31hj"
      },
      "source": [
        "len(all_adjectives)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkRofZtd33za"
      },
      "source": [
        "random.shuffle(all_sent_words)\n",
        "all_sent_words[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vHk0YX37iFA"
      },
      "source": [
        "# this function creates a list of 'feature' dictionaries from each text file with\n",
        "# information about the presence of lexicon words in the file.\n",
        "\n",
        "def find_features(document):\n",
        "    words = word_tokenize(document)\n",
        "    features = {}\n",
        "    for w in all_sent_words:\n",
        "        features[w] = (w in words)\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dQjLVU17liP"
      },
      "source": [
        "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TbrJRt64gEg"
      },
      "source": [
        "# if the above cell takes too long for you...use the following code to take a smaller random sample\n",
        "# of our sentiment lexicon. Any idea why it took so long?\n",
        "\n",
        "#random.shuffle(all_adjectives)\n",
        "#all_adjectives = all_adjectives[:5000]\n",
        "\n",
        "#featuresets = [(find_features(rev), category) for (rev, category) in documents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRTDlIrS7nOs"
      },
      "source": [
        "# this should explain why the prior cell took so long...\n",
        "\n",
        "featuresets[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPxqw3Jv7o_I"
      },
      "source": [
        "random.shuffle(featuresets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn35vpB87qMP"
      },
      "source": [
        "# and now let's separate our data into training and test cases...\n",
        "\n",
        "training_set = featuresets[:2500]\n",
        "testing_set = featuresets[2500:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMUHPFvs8DBw"
      },
      "source": [
        "## 6: Sentiment analysis with machine learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qFnh0OZ8UUi"
      },
      "source": [
        "# So how did we do?\n",
        "# let's discuss what we've done here...\n",
        "\n",
        "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
        "\n",
        "print(\"Classifier accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
        "\n",
        "classifier.show_most_informative_features(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdjHiTTn97sS"
      },
      "source": [
        "Our classifier works pretty well!! Although, your mileage may vary...Depending on the number of lexicon words you were able to incorporate, your accuracy percentage (the ratio of correctly predicted cases) should be around 80%.\n",
        "\n",
        "<br>\n",
        "\n",
        "There are many other classifying algorithms to try out, but we like this one for today. Let's figure out how to pack it away so we can test new data with it without having to retrain it over and over again. One way to do this is through a process called 'pickling'...yum!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiuO-Wp19qu7"
      },
      "source": [
        "# pickling allows us to store python objects like our classifier in byte format for simple recall\n",
        "# let's dump our classifier object into a pickle file\n",
        "\n",
        "# wb = write in bytes as opposed to strings...\n",
        "save_my_algorithm = open(\"../data/my_algorithm.pickle\",\"wb\")\n",
        "pickle.dump(classifier, save_my_algorithm)\n",
        "save_my_algorithm.close"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axvo1USUCywL"
      },
      "source": [
        "# rb = read bytes in the pickle file\n",
        "my_algorithm = open(\"../data/naivebayes.pickle\", \"rb\")\n",
        "classifier = pickle.load(my_algorithm)\n",
        "my_algorithm.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gXgU06WLeke"
      },
      "source": [
        "# let's test our algorithm with individual text snippets to see how it performs...\n",
        "\n",
        "text = all_cleaned[0]\n",
        "feature = find_features(text)\n",
        "\n",
        "#and let's read the text to get a feel\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X9NOqXoLs6j"
      },
      "source": [
        "# last but not least, let's classify the individual feature with our pickled algorithm\n",
        "# how did it do for you?\n",
        "\n",
        "classifier.classify(feature)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}